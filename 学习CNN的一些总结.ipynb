{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66892d71",
   "metadata": {},
   "source": [
    "# 帮助收敛和泛化的一些做法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f75f45",
   "metadata": {},
   "source": [
    "## 数据归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a64ef1",
   "metadata": {},
   "source": [
    "* 保持激活检查 ： 批量归一化 （正则化）\n",
    "我们要的数据是输出在底部，底层的训练比较慢，而且底层的变化带动着所有层的变化，最后层的数据往往要学习多次，这样也导致他们的收敛会变慢\n",
    "\n",
    "在求解梯度的时候，输入层的梯度是最大的\n",
    "\n",
    "我们会尝试在学习底部层的的时候避免变化顶部层，因为顶层开始收敛的比较快，训练持续的时候，顶部变化，底部变化，顶部收敛快，底部收敛慢。\n",
    "\n",
    "想法 ： 1 ， 固定小批量里面的均质和方差 ， 然后在调整学习参数，也就是超参数\n",
    "\n",
    "$$\\hat{z_i}=\\frac{z_i-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$$   （归一化）\n",
    "其中，$\\gamma$是缩放参数，$\\beta$是平移参数。\n",
    "$$y_i=\\gamma\\frac{z_i-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}+\\beta$$\n",
    "（批量归一化）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a726162",
   "metadata": {},
   "source": [
    "* 卷积层或者全连接层，输出之后直接贴了一个批量归一化，也就是在激活函数的前面 ， 之后就可以作为输入了， 把均质方差拉的比较好，不会变化太剧烈\n",
    "* 全连接层，特征维度上的\n",
    ">对于一个二维输入，每个行都是一个样本，每一列都是特征 ， 对每个特征进行运算一个均质和方差，他也会自己学习伽马和贝塔继续做校任\n",
    "卷积输入（batch_size , hight , weight , channels) <-tensorflow\n",
    "\n",
    "\n",
    ">对于1 * 1卷积的特殊情况，其实就是全连接层 ， 批量大小*高*宽其实就是每一个像素都是一个样本 ， 作用在通道层（每个像素都有通道）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273ca55",
   "metadata": {},
   "source": [
    "* 本质上有点像在通道内部模糊化，或者声音里头掺噪音 （mu -> 随机偏移） （sigma -> 随机缩放）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11315cb4",
   "metadata": {},
   "source": [
    "* 所以没必要跟dropout使用 （可以调大学习率，收敛速度会比变快，但是不会改变模型精度）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898be75",
   "metadata": {},
   "source": [
    "## 范数惩罚 ----> 权重衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c9623",
   "metadata": {},
   "source": [
    "范数 ———— 表示一个向量有多大\n",
    "L1 范数 ———— 所有权重的绝对值的和 —————— 通过小因子进行缩放\n",
    "\n",
    "L2 范数 ———— 所有权重的平方和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffaabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 范数的实现 \n",
    "def l1_Norm(model):\n",
    "    output = model(img) ; l1_lamda = 1e-3 ; l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "# L2 范数只需要把abs() 换成 pow(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed46da",
   "metadata": {},
   "source": [
    "## 丢弃法（dropout）随机的使神经元失活"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057dab1",
   "metadata": {},
   "source": [
    "nn.DropoutXd(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb871885",
   "metadata": {},
   "source": [
    "# 更加复杂的结构：深度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918c430",
   "metadata": {},
   "source": [
    "### 跳跃连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165d5fb",
   "metadata": {},
   "source": [
    "增加深度的同时也代表着训练更加收敛 ， 损失函数对参数的求导 ， 可能会因为很长的求导链式法则产生很多其他的数字 ， 这些数字可能会很小，也可能会很大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5cb311",
   "metadata": {},
   "source": [
    "这样不断的求下去，可能会导致某些参数对梯度的贡献减小，导致某一些层的训练，最后是没有效果的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9b0c3",
   "metadata": {},
   "source": [
    "所以跳跃连接就是我们直接把输入，添加到层快的一个输出中 ， 比如说直接把第一个激活函数的输出，作为第3层的输入，这样我们就缓解了梯度消失的问题\n",
    "\n",
    "我们也可以理解为创建了一个从深层参数（要跳跃的）到损失的直接路径 ， 使得他们对梯度的贡献更加直接了，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06bb6e",
   "metadata": {},
   "source": [
    "# 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e6972",
   "metadata": {},
   "source": [
    "写一个封装函数\n",
    "```python\n",
    "def init_(m):\n",
    "    if type(m)==nn.Linear :\n",
    "        nn.init.constant_(m.weight ,mean , std)\n",
    "        nn.init.zeros_()\n",
    "model.apply()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ffdab",
   "metadata": {},
   "source": [
    "共享参数_创建一个shared层\n",
    "\n",
    "shared = nn.Linear()\n",
    "\n",
    "是因为他的梯度叠加导致他对损失的贡献一直很明显，所以可以更好的拟合 ————跳跃连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b83495b",
   "metadata": {},
   "source": [
    "输入特征图的大小为H×W，卷积核的大小为K×K，则输出特征图的大小为 (H-K+1)×(W-K+1)。这种情况下，卷积层只进行特征提取，不改变特征图的深度，因为卷积核的深度与输入特征图的深度相同\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078c91f",
   "metadata": {},
   "source": [
    "如果想要保持输入特征图和输出特征图尺寸相同可以采用padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfa824",
   "metadata": {},
   "source": [
    "### 为什么1*1的卷积层比全连接层更有效率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fd6fc",
   "metadata": {},
   "source": [
    "* 输入大小为N * H * C 的1*1卷积层，输出大小为C * K个 ， K 为输出通道数\n",
    "* 输入大小为N * H * C 的全连接层 ，输出大小为 H * C * K ， K 为需要学习的偏执层（每个全连接层中的隐藏层都对应着一个偏执层\n",
    "* 卷积计算具有权重共享，虽然是1*1叫卷积，但是依旧可以在通道维度上进行权重共享 ， 针对的是每个张量再输入通道上的权重\n",
    "* 全连接层只在水平方向上具有连接性 ， 卷积层在水平和垂直 都具有连接性 ， 所以可以更好的进行拟合和泛化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37aafac",
   "metadata": {},
   "source": [
    "#### 如何构建层和块\n",
    "```python\n",
    "class MLP(nn.Module): # 层\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "    def forward(self , X):\n",
    "        return self.out(F.relu(self.hidden(X))                      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759a059",
   "metadata": {},
   "source": [
    "```python\n",
    "class MySequential(nn.Module): # 顺序块\n",
    "    def __init__(self , *args):\n",
    "        super().__init__()\n",
    "        for block in args :\n",
    "            self._modules[block] = block\n",
    "    def forward(self , X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f832f",
   "metadata": {},
   "source": [
    "#### 参数访问"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0a3c1",
   "metadata": {},
   "source": [
    "state_dict() 查看weight和bias\n",
    "\n",
    "bias引用 和 bias.data引用 bias.grad引用\n",
    "\n",
    "通过name_parameters() 返回一个(name,param.shape)字典\n",
    "\n",
    "net.add_module()可以添加一个字符串对网络的说明\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de4a08",
   "metadata": {},
   "source": [
    "#### 内置初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bcd883",
   "metadata": {},
   "source": [
    "```python\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight , mean = 0 , std = 1e-2)\n",
    "        nn.init.zeros_(m.bias) \n",
    "net.apply(init_normal) # 遍历整个神经网络\n",
    "\n",
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear :\n",
    "        nn.init.constant_(m.weight ,mean , std)\n",
    "        nn.init.zeros_()\n",
    "\n",
    "# 自定义线性层\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self , in_units , units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units , units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self , X):\n",
    "        linear = torch.matuml(X , self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792a8df",
   "metadata": {},
   "source": [
    "### 读写文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519f0fb",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.save(x , 'x-file')\n",
    "torch.load('filename')\n",
    "torch.save(net.state_dict(),'filename')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b29e7c",
   "metadata": {},
   "source": [
    "# 获取模型的子模块\n",
    "```python\n",
    "model = models.alexnet(pretrained=True)\n",
    "mode.featrues.children()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c369c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
